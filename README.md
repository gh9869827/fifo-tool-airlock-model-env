# Airlock Model Environment

A more secure, fully isolated environment for running Hugging Face models that require `trust_remote_code=True`. This architecture ensures maximum containment using a no-network Docker container, communicating only through stdin/stdout with a local FastAPI bridge.

> âš ï¸ **Note**: This setup is not intended to be scalable but provide a more secure way to run models requiring `trust_remote_code=True` by restricting network access inside the container where the model runs. The SDK communicates with the container by using `docker exec` and by using stdin and stdout.

This projects currently focuses on Phi4 models: they are lightweight, can be run on consumer GPU, capable for agentic deployments and currently requires `trust_remote_code=True`.
---

## ğŸ§© Architecture Overview

```
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚   User /   â”‚  â† External tools / apps
      â”‚   Client   â”‚  â† e.g., Python SDK, CLI tool, chatbot
      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
      â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
      â”‚  Bridge   â”‚  â† FastAPI server (127.0.0.1) â‡„ docker exec client
      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
           â”‚
      â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚  Docker Container               â”‚  â† Isolated, no network
      â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
      â”‚  â”‚  Airlocked client          â”‚ â”‚  â† Receives input from docker exec via stdin
      â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚     â‡… Communicates with Airlocked Server via http requests
      â”‚                                 â”‚     â‡¡ Returns response to Bridge via stdout
      â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
      â”‚  â”‚  Airlocked Model Server    â”‚ â”‚  â† FastAPI server (127.0.0.1):
      â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚     Persistent
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     Loads HF models, processes input
```

---

## ğŸ“ Directory Structure

```
airlock_model_env/
â”œâ”€â”€ server/            # Container-side FastAPI server that provides the Airlocked model server logic (runs inside Docker)
â”‚   â”œâ”€â”€ fastapi_server.py
â”‚   â”œâ”€â”€ llm_model.py
â”‚   â”œâ”€â”€ llm_model_phi_4.py
â”‚   â”œâ”€â”€ llm_model_phi_4_mini_instruct.py
â”‚   â”œâ”€â”€ llm_model_phi_4_multimodal_instruct.py
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ client/            # CLI entrypoint executed via `docker exec`, sends requests via stdin/stdout
â”‚   â”œâ”€â”€ run.py
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ bridge/            # Host-side FastAPI bridge server, proxies requests to Docker-contained model
â”‚   â”œâ”€â”€ fastapi_server.py
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ sdk/               # Client SDK to talk to the FastAPI bridge
â”‚   â”œâ”€â”€ client_sdk.py
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ common/            # Shared request/response schemas and data models
â”‚   â”œâ”€â”€ models.py
â”‚   â””â”€â”€ __init__.py
â””â”€â”€ README.md          # You're here!
```

---

## âš ï¸ Prerequisites

The container is assumed to have a working PyTorch environment with CUDA and cuDNN properly installed to run Hugging Face models.

## ğŸ“¦ Python Dependencies

Install essential packages for working with large language models and serving them using application servers:

```bash
python3 -m pip install --upgrade pip setuptools wheel

python3 -m pip install uvicorn fastapi

python3 -m pip install transformers accelerate bitsandbytes peft backoff flash_attn pydantic
```

### ğŸ” Package Summary

| Package        | Description                                                                 |
|----------------|-----------------------------------------------------------------------------|
| `transformers` | State-of-the-art pre-trained models for NLP, vision, and beyond.            |
| `accelerate`   | Simplifies training and inference on CPUs, GPUs, or multi-GPU setups.       |
| `bitsandbytes` | Lightweight quantization and 8-bit optimizations for large models.          |
| `peft`         | Tools for parameter-efficient fine-tuning (LoRA, AdaLoRA, etc.) of LLMs.    |
| `trl`          | Fine-tuning trainer utilities for supervised and reinforcement learning. *(if fine-tuning)* |
| `datasets`     | Hugging Face's standard for dataset handling and preprocessing. *(if fine-tuning)* |
| `backoff`      | Retry logic with exponential backoff â€” helpful for flaky APIs.              |
| `flash-attn`   | Fast CUDA-optimized attention for large transformers.                       |
| `pydantic`     | Type-validated models for structured data â€” used heavily in FastAPI.        |
| `fastapi`      | High-performance web framework for building APIs with async Python.         |
| `uvicorn`      | Lightning-fast ASGI server for serving FastAPI apps in production.          |

---

## ğŸ” Authentication

Login to Hugging Face:

```bash
huggingface-cli login
```

---

## ğŸ“¥ Download Models (Phi-4 only)

```bash
huggingface-cli download microsoft/Phi-4-mini-instruct
huggingface-cli download microsoft/Phi-4-multimodal-instruct
```

- Disconnect from the network after downloading the models

```bash
docker network disconnect bridge dev-phi
```

---

## ğŸš€ Quick Start

### 1. Start the Docker container

```bash
docker run --rm \
  --network=none \
  --name airlock \
  airlock-image
```

### 2. Run the FastAPI bridge (on host)
From project root:

```bash
uvicorn bridge.fastapi_server:app --host 127.0.0.1 --port 8000
```

### 3. Use the client SDK (or your own tool)

```python
from sdk.client_sdk import call_airlock_model_server
response = call_airlock_model_server("What is the capital of France?")
print(response)
```

---

## ğŸ”’ Security Notes
- The model runs with `trust_remote_code=True`, but is fully isolated
- No data is persisted inside the container
- Communication is limited to `stdin` and `stdout`
- Bridge can enforce access control, rate limiting, etc.

---

## ğŸ“¦ Goals
- Minimal trusted surface area
- Clean interface between components
- Easy to extend or audit

---

## âœ… License
MIT
